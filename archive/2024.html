<!doctype html>
<html lang="en" class="h-100">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="CS Katha Barta">
    <meta name="author" content="Rucha">
    <title>CS Karta Barta | Subhankar Mishra's Lab</title>
    <link href="https://www.niser.ac.in/~smishra/css/smlab.css" rel="stylesheet" type="text/css" />
</head>

<body>
    <div class="container">
        <header class="item">
            <h1>CS Katha Barta | ସଂଗଣକ ବିଜ୍ଞାନ କଥା ବାର୍ତା</h1>
        </header>
        Hosted by <a href="https://www.niser.ac.in/~smishra/"> Subhankar Mishra's Lab</a> <br>
        People -> Rucha Bhalchandra Joshi, Subhankar Mishra
        <hr />
        <h3>CS Katha Barta 2024</h3>
        <h4>Upcoming Talks</h4>
        <ol>
            <li><strong> <a href="https://sites.google.com/view/vinayakabrol/home">Dr. Vinayak Abrol</a></strong>
                <small>
                    <em> Assistant Professor, CSE IIIT Delhi</em>
                    <ul>
                        <li>Date: Mar 11-15, 2024,  hours</li>
                        <li>Title: From Randomness to Trainability in Deep Neural Networks</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> TBA </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            <li><strong> <a href="https://dcll.iiitd.edu.in">Bapi Chatterjee</a></strong>
                <small>
                    <em> Assistant Professor, CSE IIIT Delhi</em>
                    <ul>
                        <li>Date: Mar 14, 2024, 09:30 hours</li>
                        <li>Title: Dynamics of auxiliary parameters in distributed machine learning</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> Distributed systems are at the center stage of training today's machine learning models. Such settings include shared-memory and message-passing asynchrony, compression of gradients, local training to reduce communication, and combinations thereof. With problem-specific assumptions such as non-convexity and non-smoothness in place, taming the convergence of iterates under such system-dependent inconsistencies becomes challenging. In this talk, we present several algorithms with various system- and problem-generated analytical assumptions. We discuss a general strategy for constructing these algorithms drawing from their convergence theory. We discuss constructing an auxiliary global parameter in every case. We show that convergence of the distributed machine learning training algorithm can be tracked via the dynamics of the constructed parameter. </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>

            <li><strong>Dr. Thiparat Chotibut (Thip)</strong>
                <small>
                    <em> <a href="https://www.chics.ai">Chula Intelligent and Complex Systems Lab</a>, Chulalongkorn University</em>
                    <ul>
                        <li>Date: Mar 21, 2024, 09:30 hours</li>
                        <li>Title: TBA</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> TBA </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            <li><strong>Dr. Pawan Goyal</strong>
                <small>
                    <em> Associate Professor IIT Kharagpur</em>
                    <ul>
                        <li>Date: Mar 18 - 22, 2024, </li>
                        <li>Title: TBA</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> TBA </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            <li><strong> <a href="https://kracr.iiitd.edu.in/">Dr. Raghava Mutharaju</a></strong>
                <small>
                    <em> Assistant Professor, CSE IIIT Delhi</em>
                    <ul>
                        <li>Date: April 1-5, 2024</li>
                        <li>Title: TBA</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> TBA </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            <li><strong>Dr. Saket Anand</strong>
                <small>
                    <em> Associate Professor IIIT Delhi</em>
                    <ul>
                        <li>Date: April 8-12, 2024 </li>
                        <li>Title: TBA</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> TBA </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            <li><strong>Dr. Vinod Kurmi</strong>
                <small>
                    <em> Assistant Professor IISER Bhopal</em>
                    <ul>
                        <li>Date: April 15-19, 2024 </li>
                        <li>Title: TBA</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> TBA </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            
            
        </ol>
        <h4>Past Talks</h4>
        <ol reversed>
            <li><strong>Dr. Nidhi Tiwari</strong>
                <small>
                    <em>Microsoft, India</em>
                    <ul>
                        <li>Date: Jan 29, 2024, 13:30 hours</li>
                        <li>Title: Leveraging Open-Source Foundation models to develop intelligent applications
                            <a href="https://www.youtube.com/watch?v=YFFANHxUy_E">Youtube</a>
                        </li>
                        <li>
                            <details>
                                <summary> Abstract</summary>
                                <p> OpenAI ChatGPT and other foundation models have garnered widespread attention
                                    with their ability to respond effectively to a wide range of human questions,
                                    solving logical problems, providing reasoning for the solutions, generate images and
                                    so on. We all want to explore their capabilities and utilize them for developing
                                    intelligent features/products. However, we are constrained and delayed due the high
                                    cost, low training data, limited access and large size. The increasing number and
                                    variety of Open-source foundation models are good alternative for this. In this
                                    session we will look at some of the open source LLMs. We will touch upon a few ways
                                    to access, finetune and use them for projects. We will also look at some options
                                    that enable integration of LLMs in mobile applications. </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            <li><strong>Dr. Amit Chintamani Awekar</strong>
                <small>
                    <em>Associate Professor, IIT Guwahati</em>
                    <ul>
                        <li>Date: Jan 25, 2024, 09:30 hours </li>
                        <li>Title: Addressing the data bottleneck in information extraction</li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> Supervised Machine Learning tasks require annotated data for model
                                    training. Annotating large-scale data is both costly and error-prone.
                                    The annotation error issue becomes even more complex when the
                                    number of annotation labels is of the order of hundreds or thousands.
                                    As a result, absence of high-quality data becomes the real bottleneck
                                    in improving the model performance. In this talk, we will consider
                                    three scenarios for addressing the data bottleneck.
                                    <ol>
                                    <li>Data annotations are noisy. However, we cannot afford to re-
                                    annotate the whole dataset. How do we re-annotate only a part
                                    of the data?</li>
                                    <li>The annotation labels fail to capture the fine semantics of data.
                                    How do we create new annotation labels that are appropriate
                                    for our task?</li>
                                    <li>None of the existing datasets are appropriate for our particular
                                    application. How do we create new datasets from scratch or
                                    merge multiple existing datasets?</li>
                                    </ol>
                                    We will discuss these three scenarios in the context of a specific task
                                    of Information Extraction. It is the task of extracting structured
                                    information from unstructured natural language text. </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            <li><strong>Prof. Animesh Mukherjee</strong>
                <small>
                    <em>Professor, IIT Kharagpur</em>
                    <ul>
                        <li>Date: Jan 10, 2024, 08:30 hours</li>
                        <li>Title: Vulnerabilities of LLMs in hate speech detection
                            <a href="https://www.youtube.com/watch?v=oZA_G6KH-2A">Youtube</a>
                        </li>
                        <li>
                            <details>
                                <summary>Abstract</summary>
                                <p> Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select two large language models (GPT-3.5 and text-davinci) and three datasets - HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially (∼20-30%) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline (∼10-20%) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute ‘jailbreak’ prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts. </p>
                            </details>
                        </li>
                    </ul>
                </small>
            </li>
            
            

        </ol>
        <hr />
        <h3><a href="https://www.niser.ac.in/~smishra/event/cskathabarta/">CS Katha Barta</a> Past years</h3>
        <ul>
            <li><a href="https://www.niser.ac.in/~smishra/event/cskathabarta/archive/2023.html">2023</a></li>
            <li><a href="https://www.niser.ac.in/~smishra/event/cskathabarta/archive/2022.html">2022</a></li>
            <li><a href="https://www.niser.ac.in/~smishra/event/cskathabarta/archive/2021.html">2021</a></li>
        </ul>
        <hr />
        <br> <br>
        <div class="navbar">
            <a href="https://www.niser.ac.in/~smishra/index.html">Home</a>
            <a href="https://www.niser.ac.in/~smishra/people.html">People</a>
            <a href="https://www.niser.ac.in/~smishra/research.html">Research</a>
            <a href="https://www.niser.ac.in/~smishra/teaching.html">Teaching</a>
            <a href="https://www.niser.ac.in/~smishra/events.html" class="active">Events</a>
        </div>
</body>